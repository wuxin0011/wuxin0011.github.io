---
title: script å…¥é—¨ä½“éªŒ
date: 2022-05-06 15:52:40
permalink: /pages/scrapy-01
sidebar: auto
categories:
  - éšç¬”
tags:
  - python
  - scrapy
author:
  name: wuxin0011
  link: https://github.com/wuxin0011
---


python çˆ¬è™«æ¡†æ¶ ï¼Œå’Œ Java ä¸­ springï¼Œzookeeerç­‰ï¼ŒJavaScript ä¸­çš„ webpackã€ElementUI ç­‰ç­‰ç±»ä¼¼ï¼Œæ‹¥æœ‰ä¸€å¥—å®Œå–„çš„çˆ¬è™«ä½“ç³» ã€‚å¯ä»¥è¯´å®‰è£…äº†scrapy ä¸éœ€è¦ä¾èµ–å…¶ä»–ç¬¬ä¸‰æ–¹åº“å°±å¯ä»¥å®Œæˆçˆ¬è™«å·¥ä½œäº†!
<!-- more -->



# scrapyå…¥é—¨

### å‰è¨€

python çˆ¬è™«æ¡†æ¶ ï¼Œå’Œ Java ä¸­ springï¼Œzookeeerç­‰ï¼ŒJavaScript ä¸­çš„ webpackã€ElementUI ç­‰ç­‰ç±»ä¼¼ï¼Œæ‹¥æœ‰ä¸€å¥—å®Œå–„çš„çˆ¬è™«ä½“ç³» ã€‚å¯ä»¥è¯´å®‰è£…äº†scrapy ä¸éœ€è¦ä¾èµ–å…¶ä»–ç¬¬ä¸‰æ–¹åº“å°±å¯ä»¥å®Œæˆçˆ¬è™«å·¥ä½œäº†!

* [å®˜æ–¹æ–‡æ¡£](https://docs.scrapy.org/en/latest/index.html)
* [ä¸­æ–‡æ–‡æ¡£](https://www.osgeo.cn/scrapy/)
* [Github](https://github.com/scrapy/scrapy)


### å®‰è£… 



#### å¼€å§‹

```shell
# é¦–å…ˆæ‰§è¡Œè¯¥å‘½ä»¤ åˆ°pythonå®‰è£…æ–‡ä»¶å¤¹ï¼Œç„¶åè¿›å…¥scripy æ–‡ä»¶å¤¹
# ä¾‹å¦‚ D:\environment\python\Scripts>
# æ‰§è¡Œä¸‹è½½å‘½ä»¤
pip install scrapy 
# ä½¿ç”¨å›½å†…é•œåƒä¸‹è½½æé«˜è®¿é—®é€Ÿåº¦
pip install scrapy -i https://pypi.douban.com/simple
```

#### æŠ¥é”™é—®é¢˜ 

å‚è€ƒåœ°å€ ï¼š https://www.yiibai.com/scrapy/scrapy_environment.html

* å¦‚æœæ˜¯å› ä¸ºç‰ˆæœ¬é—®é¢˜æŠ¥é”™å‡çº§å®‰è£…åŒ…

```shell
python -m pip install --upgrage pip
```

* ç³»ç»Ÿå‹å·æŠ¥é”™

```shell
python install pypiwin32
```

* å¦‚æœä»¥ä¸Šæ–¹å¼å®‰è£…å¤±è´¥ä½¿ç”¨ anaconda


> æ³¨æ„ å®‰è£…æ—¶ä¸€å®šè¦å®‰è£…åˆ°pythonç¯å¢ƒç›®å½•çš„scriptsç›®å½•ä¸­ï¼Œå¦‚æœå®‰è£…åˆ°è¯¥ç›®å½•éœ€è¦é…ç½®ç¯å¢ƒå˜é‡ï¼
>


### ä½¿ç”¨


#### å‘½ä»¤ä»‹ç»

```shell
Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         å¿«é€Ÿæµ‹è¯•
  check         æ£€æŸ¥
  commands
  crawl         è¿è¡Œä¸€ä¸ªçˆ¬è™«ç¨‹åº
  edit          ç¼–è¾‘çˆ¬è™«æ–‡ä»¶
  fetch         ç±»ä¼¼äºJavaScript æµè§ˆå™¨ Fetch APIï¼Œè·å–urlè¯·æ±‚
  genspider     ä½¿ç”¨è¯¥å‘½ä»¤åˆ›å»ºä¸€ä¸ªçˆ¬è™«æ¨¡æ¿ é€šå¸¸  scrapy  genspider  æ–‡ä»¶å è¯·æ±‚åœ°å€
  list          åˆ—è¡¨å½¢å¼åˆ—å‡ºå¯ç”¨çˆ¬è™«ç¨‹åº
  parse         æµ‹è¯•åœ°å€ä¿¡æ¯ ä¾‹å¦‚ scrapy parse https://www.baidu.com
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         å‘½ä»¤æ¨¡å¼æ‰§è¡Œ
  startproject  åˆ›å»ºä¸€ä¸ªåŸºäºscrapyæ¡†æ¶ç®€å•é¡¹ç›®æ¶æ„
  version       æŸ¥çœ‹å½“å‰scrapy ç‰ˆæœ¬å·
  view          ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€ ä¾‹å¦‚ scrapy view https://www.baiu.com

Use "scrapy <command> -h" æŸ¥çœ‹è¯¥å‘½ä»¤è¯¦ç»†ä¿¡æ¯

```


æµ‹è¯•å‘½ä»¤ scrapy shell https://www.baidu.com

```shell
scrapy shell https://www.baidu.com

# è¾“å‡ºæœ€åå‡ è¡Œå†…å®¹
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x000002341C91EEF0>
[s]   item       {}
[s]   request    <GET https://www.baidu.com>
[s]   response   <200 https://www.baidu.com>
[s]   settings   <scrapy.settings.Settings object at 0x000002341C91F490>
[s]   spider     <BaiduSpider 'baidu' at 0x2341cefe320>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
```

æ“ä½œï¼ˆéœ€è¦å®‰è£…ipythonï¼‰

ä¸‹é¢ è·å–äº†responseç±»å‹ä»¥åŠ responseå¯¹è±¡å«æœ‰çš„æ–¹æ³•

```shell
In [1]: response
Out[1]: <200 https://www.baidu.com>
# è¾“å‡ºresponseå¯¹è±¡å«æœ‰çš„æ–¹æ³•
In [2]: dir(response)
Out[2]: 
['_DEFAULT_ENCODING',
 '__annotations__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__slots__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_auto_detect_fun',
 '_body',
 '_body_declared_encoding',
 '_body_inferred_encoding',
 '_cached_benc',
 '_cached_decoded_json',
 '_cached_selector',
 '_cached_ubody',
 '_declared_encoding',
 '_encoding',
 '_get_body',
 '_get_url',
 '_headers_encoding',
 '_set_body',
 '_set_url',
 '_url',
 'attributes',
 'body',
 'cb_kwargs',
 'certificate',
 'copy',
 'css',
 'encoding',
 'flags',
 'follow',
 'follow_all',
 'headers',
 'ip_address',
 'json',
 'meta',
 'protocol',
 'replace',
 'request',
 'selector',
 'status',
 'text',
 'url',
 'urljoin',
 'xpath']

```

ç®€å•æµ‹è¯•ä¸‹å‡ ä¸ªå±æ€§ä»¥åŠæ–¹æ³•

```shell
In [4]: response.url
Out[4]: 'https://www.baidu.com'

In [5]: response.xpath('//title/text()')
Out[5]: [<Selector xpath='//title/text()' data='ç™¾åº¦ä¸€ä¸‹ï¼Œä½ å°±çŸ¥é“'>]

In [6]: response.xpath('//title/text()').get()
Out[6]: 'ç™¾åº¦ä¸€ä¸‹ï¼Œä½ å°±çŸ¥é“'
# è¾“å‡ºresponseå¯¹è±¡çš„ç±»
In [7]: type(response)
Out[6]: scrapy.http.response.html.HtmlResponse

```


è·å–æ ‡é¢˜å’Œå›¾ç‰‡åœ°å€

```shell
In [10]: response.xpath('//div[@id="lg"]//img[1]')
Out[10]: [<Selector xpath='//div[@id="lg"]//img[1]' data='<img hidefocus="true" id="s_lg_img" c...'>]

In [11]: response.xpath('//div[@id="lg"]//img[1]').extract()
Out[11]: ['<img hidefocus="true" id="s_lg_img" class="index-logo-src" src="//www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png" width="270" height="129"
onerror="this.src=\'//www.baidu.com/img/flexible/logo/pc/index.png\';this.onerror=null;" usemap="#mp">']

In [12]: response.xpath('//div[@id="lg"]//img[1]/@src')
Out[12]: [<Selector xpath='//div[@id="lg"]//img[1]/@src' data='//www.baidu.com/img/PCtm_d9c8750bed0b...'>]

In [13]: response.xpath('//div[@id="lg"]//img[1]/@src').get()
Out[13]: '//www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png'

In [14]: response.xpath('//div[@id="lg"]//img[1]/@src').get().replace('//','')
Out[14]: 'www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png'

In [15]: response.xpath('//title/text()')
Out[15]: [<Selector xpath='//title/text()' data='ç™¾åº¦ä¸€ä¸‹ï¼Œä½ å°±çŸ¥é“'>]

```

å‘ç°ç™¾åº¦å¾ˆåå“¦ï¼ğŸ˜„,æä¸ªé“¾æ¥è¿˜æœ‰åŒä¸‹åˆ’çº¿


#### æºç éƒ¨åˆ†ï¼ˆå…¥é—¨ï¼‰

> ~scrapy.http.response.html.HtmlResponse~ æºä»£ç 
>


```python
from scrapy.http.response.text import TextResponse


class HtmlResponse(TextResponse):
    pass
```


```python
class TextResponse(Response):
```


è¯¥å¯¹è±¡ç»§æ‰¿ä¸ scrapy.http.response.text.TextResponse

![](https://cdn.jsdelivr.net/gh/wuxin0011/blog-resource@main/picgo/python-response-source-1.png)â€‹

ç»§æ‰¿è¯¥å¯¹ç±»æœ‰ä¸¤ä¸ªå­ç±» ä¸Šå›¾æ‰€ç¤º

å¾…ä¼šä½¿ç”¨scrapy æŠ“å–å†…å®¹ ä½¿ç”¨æœ€å¤šæ–¹æ³• ~xpath~![](https://cdn.jsdelivr.net/gh/wuxin0011/blog-resource@main/picgo/python-response-xpath.png)â€‹

æ›´å¤šå†…å®¹è¯·å‚è€ƒæºä»£ç ï¼Œæœ¬æ¬¡ä»…ä»…ä»‹ç»äº†responseå¯¹è±¡çš„æ–¹æ³•æ¥æº


#### åˆ›å»ºé¡¹ç›®


åˆå§‹åŒ–

```shell
scrapy startproject scrapy_project_demo
```

è¿›å…¥é¡¹ç›®

```shell
cd scrapy_project_demo
```

é¡¹ç›®ç»“æ„

```shell
scrapy_project_demo/
    scrapy.cfg            # é¡¹ç›®é…ç½®
    scrapy_project_demo/  # é¡¹ç›®æºæ–‡ä»¶ç›®å½•
        __init__.py
        items.py          # é¡¹ç›®ç›®æ ‡æ–‡ä»¶ç±»ä¼¼äº Java åˆ›å»ºå¥½å¯¹è±¡å±æ€§åœ¨ spiders æ–‡ä»¶å¼•ç”¨
        middlewares.py    # ä¸­é—´ä»¶
        pipelines.py      # ç®¡é“æ–‡ä»¶ 
        settings.py       # è®¾ç½®æ–‡ä»¶
        spiders/          # çˆ¬è™«æ–‡ä»¶ç›®å½• ä¸»è¦ä»£ç è·¯å¾„åŸºæœ¬ä¸Šåœ¨è¿™é‡Œå®Œæˆçš„
            __init__.py   # ç”¨æˆ·ä»£ç 
```


æŸ¥çœ‹é¡¹ç›®ç¼–å†™å¥½é¡¹ç›®å†…å®¹

```shell
D:\desktop\Learn\python\gitee-demo\spider\scrapy_project_demo>scrapy list
baidu
demo
test
```


#### èµ·æ­¥


ç¼–å†™init.py æ–‡ä»¶å†…å®¹

```python
from scrapy.spiders import Spider
class DemoSpider(Spider):
    name = "test"

    start_urls = ["https://www.baidu.com/"]

    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)
        print("é¡¹ç›®åˆå§‹åŒ–..")

    def parse(self,response ):
        print("=======================")
        print("ç™¾åº¦æ ‡ç­¾:",response.xpath('//title/text()').get())
        print("ç™¾åº¦Logo:",response.xpath('//div[@id="lg"]//img[1]/@src').get().replace('//',''))
```



åœ¨settings.pyæ–‡ä»¶ä¸­é…ç½®

* å…³é—­ robotsåè®®
* å¼€å¯è¯·æ±‚å¤´

```python
DEFAULT_REQUEST_HEADERS = {
    'Accept': '*/*',
    'referer':'https://www.baidu.com',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36 Edg/105.0.1343.42'
}

# ROBOTSTXT_OBEY = True
```


**æ³¨æ„: å¦‚æœè¿˜æ˜¯æŠ“å–ä¸åˆ°å†…å®¹ï¼Œè¯·åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ æ›´å¤šå†…å®¹**


è¿è¡Œ

```shell
scrapy crawl test
```


ç»“æœ

```shell
=======================
<200 https://www.baidu.com/>
ç™¾åº¦æ ‡ç­¾: ç™¾åº¦ä¸€ä¸‹ï¼Œä½ å°±çŸ¥é“
ç™¾åº¦Logo: www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png
```








### scrapy å·¥ä½œæµç¨‹ (æ ¸å¿ƒ)

[å‚è€ƒåœ°å€](https://www.osgeo.cn/scrapy/topics/architecture.html
)

![name](https://www.osgeo.cn/scrapy/_images/scrapy_architecture_02.png)â€‹


Scrapyä¸­çš„æ•°æ®æµç”±æ‰§è¡Œå¼•æ“æ§åˆ¶ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

1. è¿™ä¸ª [Engine](https://www.osgeo.cn/scrapy/topics/architecture.html#component-engine) è·å–è¦ä» [Spider](https://www.osgeo.cn/scrapy/topics/architecture.html#component-spiders) .
2. è¿™ä¸ª [Engine](https://www.osgeo.cn/scrapy/topics/architecture.html#component-engine) åœ¨ä¸­å®‰æ’è¯·æ±‚ [Scheduler](https://www.osgeo.cn/scrapy/topics/architecture.html#component-scheduler) å¹¶è¯·æ±‚ä¸‹ä¸€ä¸ªè¦çˆ¬è¡Œçš„è¯·æ±‚ã€‚
3. è¿™ä¸ª [Scheduler](https://www.osgeo.cn/scrapy/topics/architecture.html#component-scheduler) å°†ä¸‹ä¸€ä¸ªè¯·æ±‚è¿”å›åˆ° [Engine](https://www.osgeo.cn/scrapy/topics/architecture.html#component-engine) .
4. è¿™ä¸ª [Engine](https://www.osgeo.cn/scrapy/topics/architecture.html#component-engine) å°†è¯·æ±‚å‘é€åˆ° [Downloader](https://www.osgeo.cn/scrapy/topics/architecture.html#component-downloader) ï¼Œé€šè¿‡ [Downloader Middlewares](https://www.osgeo.cn/scrapy/topics/architecture.html#component-downloader-middleware) ï¼ˆè§ [`<span class="pre">process_request()</span>`](https://www.osgeo.cn/scrapy/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") ï¼‰
5. ä¸€æ—¦é¡µé¢å®Œæˆä¸‹è½½ï¼Œ [Downloader](https://www.osgeo.cn/scrapy/topics/architecture.html#component-downloader) ç”Ÿæˆå“åº”ï¼ˆä½¿ç”¨è¯¥é¡µï¼‰å¹¶å°†å…¶å‘é€åˆ°å¼•æ“ï¼Œå¹¶é€šè¿‡ [Downloader Middlewares](https://www.osgeo.cn/scrapy/topics/architecture.html#component-downloader-middleware) ï¼ˆè§ [`<span class="pre">process_response()</span>`](https://www.osgeo.cn/scrapy/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") ï¼‰
6. è¿™ä¸ª [Engine](https://www.osgeo.cn/scrapy/topics/architecture.html#component-engine) æ¥æ”¶æ¥è‡ªçš„å“åº” [Downloader](https://www.osgeo.cn/scrapy/topics/architecture.html#component-downloader) å¹¶å‘é€åˆ° [Spider](https://www.osgeo.cn/scrapy/topics/architecture.html#component-spiders) ç”¨äºå¤„ç†ï¼Œé€šè¿‡ [Spider Middleware](https://www.osgeo.cn/scrapy/topics/architecture.html#component-spider-middleware) ï¼ˆè§ [`<span class="pre">process_spider_input()</span>`](https://www.osgeo.cn/scrapy/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input") ï¼‰
7. è¿™ä¸ª [Spider](https://www.osgeo.cn/scrapy/topics/architecture.html#component-spiders) å¤„ç†å“åº”å¹¶å‘ [Engine](https://www.osgeo.cn/scrapy/topics/architecture.html#component-engine) ï¼Œé€šè¿‡ [Spider Middleware](https://www.osgeo.cn/scrapy/topics/architecture.html#component-spider-middleware) ï¼ˆè§ [`<span class="pre">process_spider_output()</span>`](https://www.osgeo.cn/scrapy/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output") ï¼‰
8. è¿™ä¸ª [Engine](https://www.osgeo.cn/scrapy/topics/architecture.html#component-engine) å°†å·²å¤„ç†çš„é¡¹ç›®å‘é€åˆ° [Item Pipelines](https://www.osgeo.cn/scrapy/topics/architecture.html#component-pipelines) ï¼Œç„¶åå°†å·²å¤„ç†çš„è¯·æ±‚å‘é€åˆ° [Scheduler](https://www.osgeo.cn/scrapy/topics/architecture.html#component-scheduler) å¹¶è¯·æ±‚å¯èƒ½çš„ä¸‹ä¸€ä¸ªçˆ¬è¡Œè¯·æ±‚ã€‚
9. è¯¥è¿‡ç¨‹é‡å¤ï¼ˆä»æ­¥éª¤1å¼€å§‹ï¼‰ï¼Œç›´åˆ°ä¸å†æœ‰æ¥è‡ª [Scheduler](https://www.osgeo.cn/scrapy/topics/architecture.html#component-scheduler) .
